{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLOPE for Count Data\n",
    "\n",
    "*Authors: Zachary Lau, Joey Hotz and Javier Martinez-Rodriguez*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Sorted L-One Penalized Estimation (SLOPE) is a regularization method for estimating the coefficients of a regression model which is an extension of the LASSO regression model. SLOPE is a particularly attractive choice for variable selection and regularization for data where $p \\gg n$, which makes this method quite useful for fields such as genetics.\n",
    "\n",
    "The SLOPE estimator is characterized  by the penalization term $\\sum_{i=1}^{p}\\lambda_{i}\\left|\\beta\\right|_{(i)}$. In this penalty equation, the estimated coefficients are sorted in order of magnitude, with $|\\beta|_{(1)} \\ge |\\beta|_{(2)} \\ge \\cdots \\ge |\\beta|_{(p)}$, and the corresponding penalty parameters form a non-increasing sequence, with $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{p}$ (Bogdan et al. 2015). SLOPE regularization offers finite-sample guarantees for the coefficients selected by the model. In particular, SLOPE with sequence of $\\lambda_{i}$'s inspired by the Benjamini-Hochberg (BH) procedure is proven to provide control over the false discovery rate (FDR) for Gaussian linear models (Bogdan et al. 2015).\n",
    "\n",
    "However, the efficacy of SLOPE regression has not been thoroughly discussed or investigated in the context of generalized linear models (GLMs) such as Poisson regression. This project aims to empirically evaluate the variable selection performed by SLOPE and to compare the performance of SLOPE to LASSO and Adaptive LASSO for variable selection. The central research question tackled in this report is the following: \n",
    "\n",
    "> \"**How does the performance of SLOPE regarding variable selection accuracy (FDR and power) compare to LASSO and Adaptive LASSO when applied to high-dimensional Poisson regression?**\"\n",
    "\n",
    "To address this central question, we employ simulations to compare the accuracy of variable selection (measured by FDR and power) of SLOPE against the standard and Adaptive LASSO estimators in several different experiments. These experiments vary in the dimensionality of the predictors (ratio of $p/n$), the inter-predictor correlation ($\\rho$), the sparsity ($k$) and signal strength.\n",
    "\n",
    "This document is organized into three parts: First, we provide a general background and explanation of SLOPE for multivariate linear regression models, and the effect of choosing different procedures such as the Benjamini-Hochberg to determine the penalty parameters, as discussed by Bogdan et al. (2015). Second, we adress the implementation of SLOPE for generalized linear models – particularly those with count data – as proposed by Larsson et al. (2024).  In the third section, we discuss the simulation methodology, alternative penalizations to SLOPE, and discuss the results of the simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Results\n",
    "\n",
    "### Model Selection and SLOPE\n",
    "\n",
    "SLOPE extends the LASSO penalty by introducing a sequence of non-increasing regularization parameters, resulting in the optimization problem \n",
    "$$\\min \\frac 1 2 \\|y-X\\beta\\|^2 + \\alpha \\cdot \\sum_{i=1}^p \\lambda_{i} |\\beta|_{(i)},$$\n",
    "where $|\\beta|_{(1)} \\ge |\\beta|_{(2)} \\ge \\dots \\ge |\\beta|_{(p)}$ are sorted in descending order and the sequence $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p \\ge 0$ provides non-increasing penalization parameters (Bogdan et al. 2015, 1109). SLOPE generalizes LASSO and reduces to it if all $\\lambda_i = \\lambda$.\n",
    "\n",
    "In comparison to the LASSO estimator of Tibshirani (1996), SLOPE applies larger penalties $\\lambda_i$ to coefficients with larger magnitudes. The motivation for doing this is to provide finite sample guarantees on the selected model. According to Bogdan et al. (2015), variable selection can be interpreted as multiple hypothesis testing ($H_{0j}: \\beta_j = 0$). While LASSO weakly controls the familywise error rate (FWER) at level $\\alpha \\in [0, 1]$ under an orthogonal design ($X^TX=I_p$) by setting $\\lambda = \\sigma \\Phi^{-1}(1-\\alpha/(2p))$ (Abramovich, Grinshtein and Pensky 2007), SLOPE is designed to control the FDR under an orthogonal design.\n",
    "\n",
    "In order to control the FDR, the authors propose choosing the $\\lambda_i$ sequence based on BH critical values\n",
    ". This connection is often solidified by choosing the $\\lambda_i$ sequence based on BH critical values. In the case of the Gaussian model and orthogonal designs, it is\n",
    "$$\\lambda_i = \\sigma \\Phi^{-1}\\left(1 - \\frac{q \\cdot i}{2p}\\right),$$\n",
    "where $\\sigma$ is the noise standard deviation (often assumed known), $\\Phi^{-1}$ is the standard normal quantile function, $q$ is the target FDR level, $p$ is the number of predictors, and $i$ is the rank. While related, SLOPE is not identical to BH testing; it remains a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SLOPE for Count Data\n",
    "\n",
    "The theory of SLOPE can easily be extended to GLM's. In particular, we focus on\n",
    "finding the analagous Benjamini-Hochberg coefficients in the case of a GLM.\n",
    "Recall the pdf of an exponential family model is given by \n",
    "$$ f(y) = \\exp\\left(\\frac{y\\eta - b(\\eta)}{a(\\phi)}+c(y,\\phi)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the canonical link, $\\eta = \\mathbf{X}^\\top\\beta$, so we may write\n",
    "\n",
    "$$f(y) = \\exp\\left(\\frac{yX^\\top\\beta - b(X^\\top \\beta)}{a(\\phi)}+c(y,\\phi)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we may find the MLE $\\hat\\beta$ numerically. Under appropriate\n",
    "regularity conditions, it has asymptotic distribution \n",
    "\n",
    "$$ \\sqrt{n}(\\hat\\beta-\\beta) \\leadsto \\mathcal N(0, \\mathcal I^{-1}(\\beta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathcal I(\\beta)$ is the expected Fisher information. For the\n",
    "exponential family it is given by\n",
    "\n",
    "$$\\frac{1}{a(\\phi)} \\mathbf{X}^\\top \\text{diag}(b''(\\mathbf{X}\\beta))\\mathbf{X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two simplifications to this expression\n",
    "- As is done by Bogdan et. al in their seminal work on SLOPE, we consider the\n",
    "case of an orthogonal design matrix $X$\n",
    "- We consider the distribution under the omnibus null hypothesis, that is \n",
    "$\\beta = \\vec 0$\n",
    "\n",
    "This second assumption is perhaps the most dubious, and we will revist it later.\n",
    "Nevertheless, under these simplifications we find that the Fisher information\n",
    "simplifies to \n",
    "\n",
    "$$ \\mathcal I(\\beta) = \\frac{b''(0)}{a(\\phi)}\\mathbf I $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus under $H0: \\beta = \\vec 0$, the coefficient estimates are independent\n",
    "and identically distributed with variance $\\frac{a(\\phi)}{b''(0)}$. For the\n",
    "Poisson model in particular, we have\n",
    "- $a(\\phi) = 1$\n",
    "- $b''(0) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Benjamini-Hochberg method as in Bogdan et al. this gives BH-inspired\n",
    "slope coefficients of\n",
    "$$\\lambda_j = \\sqrt{\\frac{b''(0)}{a(\\phi)}}\\phi^{-1}\\left(1-\\frac{qj}{2m}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Poisson case in particular we get \n",
    "\n",
    "$$\\lambda_j = \\phi^{-1}\\left(1-\\frac{qj}{2m}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Competing Penalizations\n",
    "\n",
    "The LASSO penalty is defined as $\\lambda \\|\\boldsymbol{\\beta}\\|_1 = \\lambda \\sum_{j=1}^{p} |\\beta_j|$, where $\\lambda \\ge 0$ is a single tuning parameter that regulates the strength of the penalty (Tibshirani 1996). As previously discussed, Bonferroni-inspired penalization of LASSO provides control over FWER. however, this extension does not apply to non-Gaussian GLMs. For this reason, our project only compares LASSO tuned through cross-validation.\n",
    "\n",
    "The Adaptive LASSO, proposed by Zou (2006), modifies the standard LASSO penalty by incorporating adaptive weights for each coefficient $\\lambda \\sum_{j=1}^{p} w_j |\\hat{\\beta}_j|$. These weights $w_j$ are determined from an initial consistent estimate of the coefficients $w_j \\propto 1/|\\hat{\\beta}_{init, j}|^{-\\gamma}$. The motivation for these weights is to assign smaller penalties to coefficients with large initial estimates while imposing larger penalties on those with smaller initial estimates.\n",
    "\n",
    "Due to its design, Zou (2006, 1424) showed that the oracle properties of Adaptive LASSO can be extended to GLMs under mild regularity conditions. This indicates that the estimator asymptotically behaves as if the true underlying model is known in advance, this method correct identification of the true set of non-zero coefficients with a probability approaching 1 as the sample size $n \\rightarrow \\ infty$. Consequently, this method serves as a benchmark for comparing the variable selection capabilities of Poisson SLOPE.\n",
    "\n",
    "Other methodologies, including Data Splitting, Multiple Data Splitting (MDS), model-X knockoff, and Gaussian mirror have been shown to effectively control FDR for GLMs. However, these approaches will not be addressed in this project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental Settings (212 words)\n",
    "\n",
    "To compare the variable selection performance of SLOPE against LASSO and Adaptive LASSO for high-dimensional data, count data is generated following a Poisson distribution with a log-linear link given by $\\log(\\lambda_i) = \\beta_0 + X_i \\beta$. The comparison is measured in terms of FDR and Power across 50 replications to ensure statistical stability. Performance is assessed using SLOPE tuned to a target FDR of $q=0.1$ via a BH-inspired $\\lambda$ sequence using 10-fold cross-validation, while LASSO and Adaptive LASSO tune $\\lambda$ using 10-fold cross-validation minimizing Poisson deviance.\n",
    "\n",
    "The chosen experimental settings are conditions to challenge variable selection. The dimensionality cases include scenarios where predictors outnumber observations, are equal and are fewer ($p \\in \\{500, 1000, 2000\\}$ with $n=1000$). Predictor correlation is evaluated with none, medium, and high correlation ($\\rho \\in \\{0, 0.5, 0.8\\}$). This is particularly relevant since correlation among regressors introduces complications in the selection of variables.\n",
    "\n",
    "The underlying sparsity level ($k \\in \\{10, 20, 50, 100\\}$ active predictors) is evaluated at different levels to see how methods cope as the number of true signals increases relative to the total predictors. Finally, simulating both \"Weak\" and \"Strong\" signal strengths (magnitudes of non-zero $\\beta_j$) tests the Power of each method to detect subtle versus obvious effects. The code below implements the generative model for the discussed settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ----\n",
    "## Packages to use ----\n",
    "if (!require(\"pacman\")) install.packages(\"pacman\")\n",
    "if (!require(\"mytidyfunctions\")) remotes::install_github(\"JavierMtzRdz/mytidyfunctions\")\n",
    "\n",
    "pacman::p_load(tidyverse, janitor, \n",
    "               SLOPE, glmnet, MASS,\n",
    "              # mytidyfunctions,\n",
    "               progress,\n",
    "               patchwork, here)\n",
    "\n",
    "## Load fonts ----\n",
    "extrafont::loadfonts(quiet = TRUE)\n",
    "\n",
    "## Set theme ------\n",
    "# mytidyfunctions::set_mytheme(text = element_text(family = \"Lato\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative models\n",
    "## Simulation Parameters\n",
    "n <- 5000         \n",
    "p_values <- c(500, 1000, 2000) \n",
    "rho_values <- c(0, 0.5, 0.8)  \n",
    "k_values <- c(10, 20, 50, 100) # Non-zero betas\n",
    "signal_strengths <- list( \n",
    "  weak = list(beta_min = 0.1, beta_max = 0.5),\n",
    "  strong = list(beta_min = 0.5, beta_max = 1.5)\n",
    ")\n",
    "R <- 1 \n",
    "q_fdr <- 0.1       # q parameter for SLOPE\n",
    "adapt_lasso_gamma <- 1 # ALasso weights\n",
    "beta0 <- 0.5\n",
    "\n",
    "set.seed(538)\n",
    "\n",
    "## Generative model\n",
    "generate_data <- function(n, p, rho, k, signal_info, beta0) {\n",
    "\n",
    "  beta_true <- numeric(p)\n",
    "\n",
    "  if (k > 0) {\n",
    "    non_zero_indices <- sample(1:p, k)\n",
    "    magnitudes <- runif(k, min = signal_info$beta_min, max = signal_info$beta_max)\n",
    "    signs <- sample(c(-1, 1), k, replace = TRUE)\n",
    "    beta_true[non_zero_indices] <- magnitudes * signs\n",
    "  }\n",
    "  true_support <- which(beta_true != 0)\n",
    "\n",
    "  # Generate X\n",
    "  Sigma <- matrix(rho, nrow = p, ncol = p)\n",
    "  diag(Sigma) <- 1\n",
    "  X <- MASS::mvrnorm(n = n, mu = rep(0, p), Sigma = Sigma)\n",
    "  X <- scale(X)\n",
    "\n",
    "  # Count response \n",
    "  lambda <- exp(beta0 + X %*% beta_true)\n",
    "  # lambda <- pmin(lambda, some_large_value)\n",
    "  y <- rpois(n, lambda)\n",
    "\n",
    "  return(list(X = X, y = y, beta_true = beta_true, true_support = true_support, beta0 = beta0, lambda = lambda))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate FDR and Power\n",
    "calculate_metrics <- function(selected_indices, true_indices, p) {\n",
    "  true_positives <- length(intersect(selected_indices, true_indices))\n",
    "  false_positives <- length(setdiff(selected_indices, true_indices))\n",
    "  # Power\n",
    "  power <- ifelse(length(true_indices) == 0, NA, true_positives / length(true_indices))\n",
    "  # FDR\n",
    "  fdr <- ifelse((true_positives + false_positives) == 0, 0, false_positives / (true_positives + false_positives))\n",
    "  return(list(FDR = fdr, Power = power, TP = true_positives, FP = false_positives, Selected_Count = length(selected_indices)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop ---\n",
    "\n",
    "results_list <- list()\n",
    "total_runs <- length(p_values) * length(rho_values) * length(k_values) * length(signal_strengths) * R\n",
    "cli::cli_progress_bar(\"Cleaning data\", total = total_runs)\n",
    "iter <- 0\n",
    "\n",
    "for (p in p_values) {\n",
    "  for (rho in rho_values) {\n",
    "    for (k in k_values) {\n",
    "      for (signal_name in names(signal_strengths)) {\n",
    "        signal_info <- signal_strengths[[signal_name]]\n",
    "\n",
    "        for (rep in 1:R) {\n",
    "          cli::cli_progress_update()\n",
    "          iter <- iter + 1\n",
    "\n",
    "          # Generate Data\n",
    "          sim_data <- generate_data(n = n, p = p, rho = rho, k = k,\n",
    "                                    signal_info = signal_info, beta0 = beta0)\n",
    "          X <- sim_data$X\n",
    "          y <- sim_data$y\n",
    "          lambda_val <- sim_data$lambda\n",
    "          true_beta <- sim_data$true_support # Indices of non-zero elements in beta_true\n",
    "          true_support <- which(abs(true_betaa) > 1e-6)\n",
    "          # Models\n",
    "\n",
    "          # SLOPE\n",
    "          selected_slope <- integer(0) # Initialize as empty\n",
    "          fdr_slope <- NA\n",
    "          power_slope <- NA\n",
    "          slope_error <- FALSE\n",
    "          slope_fit <- SLOPE::SLOPE(X, y, family = \"poisson\", q = q_fdr, lambda = \"bh\", alpha = 1 / sqrt(n))\n",
    "          slope_coeffs <- coef(slope_fit)\n",
    "\n",
    "          selected_slope <- which(slope_coeffs[-1] != 0)\n",
    "          metrics_slope <- calculate_metrics(selected_slope, true_support, p)\n",
    "          fdr_slope <- metrics_slope$FDR\n",
    "          power_slope <- metrics_slope$Power\n",
    "\n",
    "\n",
    "          # LASSO \n",
    "          selected_lasso <- integer(0)\n",
    "          fdr_lasso <- NA\n",
    "          power_lasso <- NA\n",
    "          lasso_error <- FALSE\n",
    "          cv_lasso_fit <- cv.glmnet(X, y, family = \"poisson\", alpha = 1, standardize = FALSE)\n",
    "          lasso_coeffs <- coef(cv_lasso_fit, s = \"lambda.min\")\n",
    "          selected_lasso <- which(lasso_coeffs[-1] != 0) \n",
    "          metrics_lasso <- calculate_metrics(selected_lasso, true_support, p)\n",
    "          fdr_lasso <- metrics_lasso$FDR\n",
    "          power_lasso <- metrics_lasso$Power\n",
    "\n",
    "\n",
    "          # Adaptive LASSO\n",
    "          selected_adapt <- integer(0)\n",
    "          fdr_adapt <- NA\n",
    "          power_adapt <- NA\n",
    "          adapt_error <- FALSE\n",
    "\n",
    "          cv_ridge_fit <- cv.glmnet(X, y, family = \"poisson\", alpha = 0, standardize = FALSE) \n",
    "          ridge_coeffs <- coef(cv_ridge_fit, s = \"lambda.min\")[-1] \n",
    "\n",
    "          # Calculate weights \n",
    "          weights <- 1 / (abs(ridge_coeffs) + .Machine$double.eps)^adapt_lasso_gamma\n",
    "          weights <- pmin(weights, 1e10) \n",
    "\n",
    "          # Fit weighted LASSO using CV\n",
    "          cv_adapt_fit <- cv.glmnet(X, y, family = \"poisson\", alpha = 1,\n",
    "                                      penalty.factor = weights, standardize = FALSE)\n",
    "          adapt_coeffs <- coef(cv_adapt_fit, s = \"lambda.min\")\n",
    "          selected_adapt <- which(adapt_coeffs[-1] != 0) \n",
    "      \n",
    "          metrics_adapt <- calculate_metrics(selected_adapt, true_support, p)\n",
    "          fdr_adapt <- metrics_adapt$FDR\n",
    "          power_adapt <- metrics_adapt$Power\n",
    "\n",
    "          # Save results\n",
    "          results_list[[iter]] <- data.frame(\n",
    "            n = n, p = p, rho = rho, k = k, signal = signal_name, replication = rep,\n",
    "            Method = c(\"SLOPE\", \"LASSO\", \"AdaptiveLASSO\"),\n",
    "            FDR = c(fdr_slope, fdr_lasso, fdr_adapt),\n",
    "            Power = c(power_slope, power_lasso, power_adapt),\n",
    "            SelectedCount = c(length(selected_slope), length(selected_lasso), length(selected_adapt))\n",
    "          )\n",
    "\n",
    "        }}}}} \n",
    "cli::cli_progress_done()\n",
    "\n",
    "final_results <- bind_rows(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fit <- SLOPE::trainSLOPE(X, y, family = \"poisson\", q = c(0.1, 0.2))\n",
    "slope_coeffs <- coef(slope_fit)\n",
    "plot(slope_fit) +\n",
    "  guides(color = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 1000\n",
    "p <- n / 2\n",
    "data <- SLOPE:::randomProblem(n = n, p = 1000, q = 0.3, response = \"gaussian\", alpha = 1)\n",
    "X <- data$x\n",
    "y <- data$y\n",
    "true_support <- which(abs(data$beta) > 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- SLOPE(X,\n",
    "  y,\n",
    "  lambda = \"gaussian\",\n",
    "  solver = \"admm\",\n",
    "  q = 0.1,\n",
    "  alpha = 1 / sqrt(n)\n",
    ")\n",
    "\n",
    "selected_slope <- which(fit$nonzeros)\n",
    "V <- length(setdiff(selected_slope, signals))\n",
    "R <- length(selected_slope)\n",
    "\n",
    "slope_coeffs <- coef(fit)\n",
    "selected_slope <- which(slope_coeffs[-1] != 0)\n",
    "true_support <- which(abs(problem$beta) > 1e-6)\n",
    "calculate_metrics(selected_slope, true_support, 500)\n",
    "V/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOPE:::randomProblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results <- bind_rows(results_list)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman::p_load(SLOPE)\n",
    "# proportion of real signals\n",
    "q <- seq(0.05, 0.5, length.out = 20)\n",
    "fdr <- double(length(q))\n",
    "set.seed(1)\n",
    "\n",
    "for (i in seq_along(q)) {\n",
    "  n <- 1000\n",
    "  p <- n / 2\n",
    "  alpha <- 1\n",
    "  problem <- SLOPE:::randomProblem(n, p, q = q[i], alpha = alpha)\n",
    "\n",
    "  x <- problem$x\n",
    "  y <- problem$y\n",
    "  signals <- problem$nonzero\n",
    "\n",
    "  fit <- SLOPE(x,\n",
    "    y,\n",
    "    lambda = \"bh\",\n",
    "    solver = \"admm\",\n",
    "    q = 0.05,\n",
    "    alpha = alpha / sqrt(n)\n",
    "  )\n",
    "\n",
    "  selected_slope <- which(fit$nonzeros)\n",
    "  V <- length(setdiff(selected_slope, signals))\n",
    "  R <- length(selected_slope)\n",
    "  fdr[i] <- V / R\n",
    "  slope_coeffs <- coef(fit)\n",
    "  selected_slope <- which(slope_coeffs[-1] != 0)\n",
    "  true_support <- which(abs(problem$beta) > 1e-6)\n",
    "  metrics_slope <- calculate_metrics(selected_slope, true_support, p)\n",
    "  fdr[i] <- metrics_slope$FDR\n",
    "}\n",
    "\n",
    "library(ggplot2)\n",
    "\n",
    "ggplot(mapping = aes(q, fdr)) +\n",
    "  geom_hline(yintercept = 0.1, lty = 3) +\n",
    "  geom_line() +\n",
    "  geom_point() +\n",
    "  theme_minimal() +\n",
    "  labs(y = \"False Discovery Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives / length(selected_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Abramovich, Felix, Vadim Grinshtein, and Marianna Pensky. 2007. “On Optimality of Bayesian Testimation in the Normal Means Problem.” The Annals of Statistics 35 (5): 2261–86.\n",
    "\n",
    "Bogdan, Małgorzata, Ewout van den Berg, Chiara Sabatti, Weijie Su, and Emmanuel J. Candès. 2015. “Slope—Adaptive Variable Selection Via Convex Optimization.” The Annals of Applied Statistics 9 (3): 1103–40.\n",
    "\n",
    "Larsson, Johan, Jonas Wallin, Malgorzata Bogdan, Ewout van den Berg, Chiara Sabatti, Emmanuel Candes, Evan Patterson, et al. 2024. “SLOPE: Sorted L1 Penalized Estimation.” R. CRAN. https://cran.r-project.org/web/packages/SLOPE/index.html.\n",
    "\n",
    "Lin, Buyu. 2024. “Problems in Variable Selection: False Discovery Rate Control and Variational Inference,” May. https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37378822.\n",
    "\n",
    "Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267–88.\n",
    "\n",
    "Zou, Hui. 2006. “The Adaptive Lasso and Its Oracle Properties.” Journal of the American Statistical Association 101 (476): 1418–29. https://doi.org/10.1198/016214506000000735."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
